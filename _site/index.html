<!DOCTYPE html>
<html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    Decentralized Diffusion
  
</title>
<meta name="author" content="David McAllister">
<meta name="description" content="Train diffusion models across many GPU clusters without networking bottlenecks.">

  <meta name="keywords" content="decentralized diffusion, diffusion models, decentralized, federated learning, image generation">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Bootstrap Table -->


<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">



<!-- Styles -->

<!-- pseudocode -->



  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="http://0.0.0.0:8080/">

<!-- Dark Mode -->
<script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script>


<!-- GeoJSON support via Leaflet -->

  <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.css" integrity="sha256-q9ba7o845pMPFU+zcAll8rv+gC+fSovKsOoNQ6cynuQ=" crossorigin="anonymous">


<!-- diff2html -->

  <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github.min.css" integrity="sha256-Oppd74ucMR5a5Dq96FxjEzGF7tTw2fZ/6ksAqDCM8GY=" crossorigin="anonymous" media="screen and (prefers-color-scheme: light)">
  <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github-dark.min.css" integrity="sha256-nyCNAiECsdDHrr/s2OQsp5l9XeY2ZJ0rMepjCT2AkBk=" crossorigin="anonymous" media="screen and (prefers-color-scheme: dark)">
  <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/css/diff2html.min.css" integrity="sha256-IMBK4VNZp0ivwefSn51bswdsrhk0HoMTLc2GqFHFBXg=" crossorigin="anonymous">





  <link defer rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css">


    
      <!-- Medium Zoom JS -->
      <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
      <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>
    
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    
  
    <!-- MathJax -->
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          tags: 'ams',
        },
      };
    </script>
    <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script>
  


    
  <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script>
  
    <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script>
  
  <script>
    let mermaidTheme = determineComputedTheme();

    /* Create mermaid diagram as another node and hide the code block, appending the mermaid node after it
       this is done to enable retrieving the code again when changing theme between light/dark */
    document.addEventListener('readystatechange', () => {
      if (document.readyState === 'complete') {
        document.querySelectorAll('pre>code.language-mermaid').forEach((elem) => {
          const svgCode = elem.textContent;
          const backup = elem.parentElement;
          backup.classList.add('unloaded');
          /* create mermaid node */
          let mermaid = document.createElement('pre');
          mermaid.classList.add('mermaid');
          const text = document.createTextNode(svgCode);
          mermaid.appendChild(text);
          backup.after(mermaid);
        });

        mermaid.initialize({ theme: mermaidTheme });

        /* Zoomable mermaid diagrams */
        if (typeof d3 !== 'undefined') {
          window.addEventListener('load', function () {
            var svgs = d3.selectAll('.mermaid svg');
            svgs.each(function () {
              var svg = d3.select(this);
              svg.html('<g>' + svg.html() + '</g>');
              var inner = svg.select('g');
              var zoom = d3.zoom().on('zoom', function (event) {
                inner.attr('transform', event.transform);
              });
              svg.call(zoom);
            });
          });
        }
      }
    });
  </script>


    
  <!-- diff2html doesn't go well with Bootstrap Table -->
  <script src="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/js/diff2html-ui.min.js" integrity="sha256-eU2TVHX633T1o/bTQp6iIJByYJEtZThhF9bKz/DcbbY=" crossorigin="anonymous"></script>
  <script>
    let diff2HtmlTheme = determineComputedTheme();

    /* Create diff2html as another node and hide the code block, appending the diff2html node after it
       this is done to enable retrieving the code again when changing theme between light/dark */
    document.addEventListener('readystatechange', () => {
      if (document.readyState === 'complete') {
        document.querySelectorAll('pre>code.language-diff2html').forEach((elem) => {
          const textData = elem.textContent;
          const backup = elem.parentElement;
          backup.classList.add('unloaded');
          /* create diff node */
          let diffElement = document.createElement('div');
          diffElement.classList.add('diff2html');
          backup.after(diffElement);
          const configuration = { colorScheme: diff2HtmlTheme, drawFileList: true, highlight: true, matching: 'lines' };
          const diff2htmlUi = new Diff2HtmlUI(diffElement, textData, configuration);
          diff2htmlUi.draw();
        });
      }
    });
  </script>


    
  <script src="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.js" integrity="sha256-MgH13bFTTNqsnuEoqNPBLDaqxjGH+lCpqrukmXc8Ppg=" crossorigin="anonymous"></script>
  <script>
    /* Create leaflet map as another node and hide the code block, appending the leaflet node after it */
    document.addEventListener('readystatechange', () => {
      if (document.readyState === 'complete') {
        document.querySelectorAll('pre>code.language-geojson').forEach((elem) => {
          const jsonData = elem.textContent;
          const backup = elem.parentElement;
          backup.classList.add('unloaded');
          /* create leaflet node */
          let mapElement = document.createElement('div');
          mapElement.classList.add('map');
          backup.after(mapElement);

          var map = L.map(mapElement);
          L.tileLayer('https://tile.openstreetmap.org/{z}/{x}/{y}.png', {
            maxZoom: 19,
            attribution: '&copy; <a href="http://www.openstreetmap.org/copyright">OpenStreetMap</a>',
          }).addTo(map);
          let geoJSON = L.geoJSON(JSON.parse(jsonData)).addTo(map);
          map.fitBounds(geoJSON.getBounds());
        });
      }
    });
  </script>


    
  <script defer src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js" integrity="sha256-0q+JdOlScWOHcunpUk21uab1jW7C1deBQARHtKMcaB4=" crossorigin="anonymous"></script>
  <script>
    $(document).ready(function () {
      var $canvas = null,
        $this = null,
        _ctx = null,
        _text = '';
      $('.language-chartjs').each(function () {
        $this = $(this);
        $canvas = $('<canvas></canvas>');
        _text = $this.text();
        $this.text('').append($canvas);
        _ctx = $canvas.get(0).getContext('2d');
        _ctx && _text && new Chart(_ctx, JSON.parse(_text)) && $this.attr('data-processed', true);
      });
    });
  </script>


    
  <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js" integrity="sha256-QvgynZibb2U53SsVu98NggJXYqwRL7tg3FeyfXvPOUY=" crossorigin="anonymous"></script>
  
  <script>
    let echartsTheme = determineComputedTheme();

    /* Create echarts chart as another node and hide the code block, appending the echarts node after it
       this is done to enable retrieving the code again when changing theme between light/dark */
    document.addEventListener('readystatechange', () => {
      if (document.readyState === 'complete') {
        document.querySelectorAll('pre>code.language-echarts').forEach((elem) => {
          const jsonData = elem.textContent;
          const backup = elem.parentElement;
          backup.classList.add('unloaded');
          /* create echarts node */
          let chartElement = document.createElement('div');
          chartElement.classList.add('echarts');
          backup.after(chartElement);

          /* create echarts */
          if (echartsTheme === 'dark') {
            var chart = echarts.init(chartElement, 'dark-fresh-cut');
          } else {
            var chart = echarts.init(chartElement);
          }

          chart.setOption(JSON.parse(jsonData));
          window.addEventListener('resize', function () {
            chart.resize();
          });
        });
      }
    });
  </script>


    
  <script defer src="https://cdn.jsdelivr.net/npm/vega@5.27.0/build/vega.min.js" integrity="sha256-Yot/cfgMMMpFwkp/5azR20Tfkt24PFqQ6IQS+80HIZs=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/vega-lite@5.16.3/build/vega-lite.min.js" integrity="sha256-TvBvIS5jUN4BSy009usRjNzjI1qRrHPYv7xVLJyjUyw=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/vega-embed@6.24.0/build/vega-embed.min.js" integrity="sha256-FPCJ9JYCC9AZSpvC/t/wHBX7ybueZhIqOMjpWqfl3DU=" crossorigin="anonymous"></script>

  <script>
    let vegaTheme = determineComputedTheme();

    /* Create vega lite chart as another node and hide the code block, appending the vega lite node after it
       this is done to enable retrieving the code again when changing theme between light/dark */
    document.addEventListener('readystatechange', () => {
      if (document.readyState === 'complete') {
        document.querySelectorAll('pre>code.language-vega_lite').forEach((elem) => {
          const jsonData = elem.textContent;
          const backup = elem.parentElement;
          backup.classList.add('unloaded');
          /* create vega lite node */
          let chartElement = document.createElement('div');
          chartElement.classList.add('vega-lite');
          backup.after(chartElement);

          /* Embed the visualization in the container */
          if (vegaTheme === 'dark') {
            vegaEmbed(chartElement, JSON.parse(jsonData), { theme: 'dark' });
          } else {
            vegaEmbed(chartElement, JSON.parse(jsonData));
          }
        });
      }
    });
  </script>


    
  <script defer src="https://tikzjax.com/v1/tikzjax.js" integrity="sha256-+1qyucCXRZJrCg3lm3KxRt/7WXaYhBid4/1XJRHGB1E=" crossorigin="anonymous"></script>


    
  <script src="/assets/js/typograms.js?63f3caa50c7a9624f953b3aec207afa6"></script>

  <script>
    /* Create typogram as another node and hide the code block, appending the typogram node after it
       this is done to enable retrieving the code again when changing theme between light/dark */
    document.addEventListener('readystatechange', () => {
      if (document.readyState === 'complete') {
        document.querySelectorAll('pre>code.language-typograms').forEach((elem) => {
          const texData = elem.textContent;
          const parent = elem.parentElement.parentElement;
          /* create typograms node */
          let typogram = document.createElement('pre');
          typogram.classList.add('typogram');
          const svg = create('\n' + texData, 0.3, false);
          typogram.appendChild(svg);
          parent.appendChild(typogram);
          parent.removeChild(elem.parentElement);
        });
      }
    });
  </script>


    

  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>



<!-- Bootstrap Table -->


<!-- Load Common JS -->
<script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script>
<script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script>
<script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script>



    

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
      <!-- Page/Post style -->
      <style type="text/css">
        .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
} .highlight-python {
  color: #9c4a9c;
} .highlight-comment {
  color: #a31515;
}

      </style>
    
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">
      {
            "title": "Decentralized Diffusion Models",
            "description": "Train diffusion models across many GPU clusters without networking bottlenecks.",
            "published": "January 09, 2025",
            "authors": [
              
              {
                "author": "David McAllister",
                "authorURL": "https://mcallisterdavid.com/",
                "affiliations": [
                  {
                    "name": "UC Berkeley",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Matthew Tancik",
                "authorURL": "https://www.matthewtancik.com/about-me",
                "affiliations": [
                  {
                    "name": "Luma AI",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Jiaming Song",
                "authorURL": "https://tsong.me/",
                "affiliations": [
                  {
                    "name": "Luma AI",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Angjoo Kanazawa",
                "authorURL": "https://people.eecs.berkeley.edu/~kanazawa/",
                "affiliations": [
                  {
                    "name": "UC Berkeley",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script>
  </d-front-matter>

  
    <!-- Header -->
    
      <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation">
    <div class="container">
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
          
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>

    

    <!-- Content -->
    <div class="post distill">
      <d-title>
        <h1>Decentralized Diffusion Models</h1>
        <p>Train diffusion models across many GPU clusters without networking bottlenecks.</p>
      </d-title>
      
        <d-byline></d-byline>
      

      <d-article>
        
          <d-contents>
            <nav class="l-text figcaption">
              <h3>Contents</h3>
              
                <div>
                  <a href="#simple-intuitions-for-diffusion-and-flow-models">Simple Intuitions for Diffusion and Flow Models</a>
                </div>
                
              
                <div>
                  <a href="#decentralized-diffusion-models">Decentralized Diffusion Models</a>
                </div>
                
              
                <div>
                  <a href="#why-ddms">Why DDMs?</a>
                </div>
                
              
                <div>
                  <a href="#simple-implementation">Simple Implementation</a>
                </div>
                
              
            </nav>
          </d-contents>
        
        <div style="text-align: center; margin-bottom: 20px;">
  <a href="https://arxiv.org/pdf/2501.05450" class="btn btn-lg z-depth-0" role="button" style="text-decoration: none; border: 1px solid #ccc; margin: 0 5px; padding: 10px 20px;" rel="external nofollow noopener" target="_blank">
    <i class="fas fa-file-pdf"></i> Paper
  </a>
  <a href="https://arxiv.org/abs/2501.05450" class="btn btn-lg z-depth-0" role="button" style="text-decoration: none; border: 1px solid #ccc; margin: 0 5px; padding: 10px 20px;" rel="external nofollow noopener" target="_blank">
    <i class="ai ai-arxiv"></i> arXiv
  </a>
</div>

<div class="fake-img l-body" style="margin-bottom: 0;">
  <img src="/assets/img/decentralized_diffusion/teaser_images.jpg" alt="DDM Overview" style="width: 100%; height: auto;">
</div>
<div class="caption" style="margin-top: 5px;">
    Some samples from our largest Decentralized Diffusion Model, pretrained with just eight independent GPU nodes in less than a week.
</div>

<!-- ## Motivation -->

<p>State of the art image and video diffusion models train on thousands of GPUs. They distribute computation then synchronize gradients across them at each optimization step. This incurs a massive networking load, which means that training clusters must live in centralized facilities with specialized networking hardware and enormous power delivery systems.</p>

<p>This is cost-prohibitive. Academic labs can’t build specialized clusters with custom networking fabrics. Even large companies struggle as they hit fundamental limits on power delivery and networking bandwidth when scaling to many thousands of GPUs. In both cases, networking is the critical bottleneck: training clusters need constant, high-bandwidth communication throughout the entire system. A segmented network load where independent clusters communicate internally but not among each other makes it possible to use compute where it’s available, whether in different datacenters or across the internet.</p>

<p><b>Decentralized Diffusion Models</b> (DDMs) tackle this problem. Our new method trains a series of expert diffusion models, each in communication isolation from one another. This means we can train them in different locations and on different hardware. At inference time, they ensemble through a lightweight learned router. We show that this ensemble collectively optimizes the same objective as a single diffusion model trained over the whole dataset (a monolithic model). It even outperforms monolithic diffusion models FLOP-for-FLOP, leveraging sparse computation at train and test time. Crucially, DDMs scale gracefully to billions of parameters and produce great results with reduced pretraining budgets. See some results below from a model pretrained with just eight independent GPU nodes in less than a week.
<!-- <br> <br> --></p>

<!-- <div class="fake-img l-page" style="margin-bottom: 0;">
  <img src="/assets/img/decentralized_diffusion/teaser_images.jpg" alt="DDM Overview" style="width: 100%; height: auto;">
</div>
<div class="caption l-page" style="margin-top: 5px;">
    Some samples from our largest Decentralized Diffusion Model, pretrained with just eight independent GPU nodes in less than a week.
</div> -->

<p>In this post, we present a simple, geometrically intuitive view on diffusion and flow models from which Decentralized Diffusion Models arrive naturally. We also highlight their compromise-free performance and implications for training hardware. DDMs make possible <b>simpler training systems</b> that produce <b>better models</b>.</p>

<h2 id="simple-intuitions-for-diffusion-and-flow-models">Simple Intuitions for Diffusion and Flow Models</h2>

<div class="l-body">
  <iframe src="/assets/plotly/plot_three.html" frameborder="0" scrolling="no" height="620px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>

<div class="caption">
    <b>Diffusion models can be geometrically intuitive.</b> We aim to show some of these ideas in this post.
</div>

<p>Diffusion models and rectified flows can be seen as special cases of flow matching<d-cite key="lipman2023flowmatchinggenerativemodeling"></d-cite>, so we use the FM framework to explain DDMs. Most perspectives on diffusion models and flow matching focus on the forward corruption process and the paths it samples for each training example. Let’s instead focus on the training/regression target of these models: the marginal flow. They all minimize the difference between their predictions and the marginal flow.</p>

<div class="l-body" style="text-align: center; margin-top: 0px;">
  <img src="/assets/img/decentralized_diffusion/marginal_flow_int.svg" alt="DDM Overview" style="width: 80%; height: auto;">
</div>

<!-- computed analytically over all samples $$x_0$$ in a dataset $$\mathcal{X}$$ -->

<p>The marginal flow, \(u_t(x_t)\), represents a vector field at each timestep that transports from \(x_t\), a noisy variable, to the data distribution ($x_t$ at $t=0$). When we train with flow matching, we regress the marginal flow into a model (e.g., a Diffusion Transformer<d-cite key="peebles2023scalablediffusionmodelstransformers"></d-cite>) that can sample the data distribution. The marginal flow in its analytical form is an expectation over \(x_0\) data samples. That is, the marginal flow is linear. For any given \(x_t\), it points toward the data distribution from \(x_t\). In high dimensions with many data points, this is intractable to compute. Instead, diffusion models compress this complex system into a neural network through flow matching.</p>

<div class="l-body" style="text-align: center;">
  <img src="/assets/img/decentralized_diffusion/marginal_flow_sum.svg" alt="DDM Overview" style="width: 80%; height: auto;">
</div>

<p>Let’s rewrite the marginal flow as a sum over a discrete dataset for clarity. $q(x_0)$ is a constant now. It’s now easy to see that the marginal flow is just a weighted average of the paths from $x_t$ to each data point, $u_t(x_t|x_0)$. Each path $u_t(x_t|x_0)$ is called a “conditional flow,” pointing from $x_t$ to a specific data sample $x_0$. We marginalize over these conditional flows to get the marginal flow. The weights of each path are determined by the normalized probability of drawing $x_t$ from a Gaussian centered at each $x_0$ sample, $p_t(x_t|x_0)$.</p>

<p>Sampling from the marginal flow is simple. At the maximum timestep $t=1$, \(x_t\) is drawn from the Gaussian distribution. Then, we can transport \(x_t\) to a sample from the data distribution by integrating the marginal flow backwards in time. This just means taking steps in the direction of the marginal flow at progressively decreasing timesteps. In other words, just keep taking small steps toward a weighted average of the data points and you’ll converge to a sample. Machine learning is effective at learning these weighted averages through reconstruction objectives. The meat of this interpretation is not new—it’s highly related to score matching<d-cite key="song2021scorebasedgenerativemodelingstochastic"></d-cite>, SDEs and Tweedie’s formula. These connections are covered much more thoroughly in this <a href="https://diffusionflow.github.io" rel="external nofollow noopener" target="_blank">blog post</a>.</p>

<p>We highlight a new interpretation because it compactly motivates DDMs. Our interpretation is maybe the simplest way to understand the main ideas of this family of models. It also shows that these models can be geometrically intuitive. Since we can compute the marginal flow analytically over small datasets, we can visualize it interactively in 2D. We made the plot below to show how the components of flow-based models interact.</p>

<p><b>In the following live plot:</b></p>
<ul>
  <li>Data points ($x_0$ samples) are <b style="color: #323083;">dark blue</b>.</li>
  <li>Each path/conditional flow, $u_t(x_t|x_0)$, is drawn in <b style="color: #2cc779;">turquoise</b> and its opacity represents its weight (“path weight” above).</li>
  <li>The noisy latent ($x_t$) is <b style="color: #F84643;">red</b>. Drag it around to see how each training example affects the denoising path from $x_t$ to the data distribution at different values of $t$.</li>
  <li>The <b style="color: #F84643;">red</b> dotted line shows the predicted denoising path, AKA the marginal flow, evaluated analytically that points from $x_t$ to the weighted average of the data, $\hat{x}_0$.</li>
  <li>The <b>slider</b> below simulates the denoising (decreasing $t$) and the noising (increasing $t$) processes.</li>
</ul>

<div class="l-page">
  <iframe src="/assets/plotly/plot_one.html" frameborder="0" scrolling="no" height="620px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>

<!-- <br style="margin: 6px 0;"> -->
<p><br>
Since the marginal flow is defined at each timestep, the slider updates the timestep t. $x_t$ will be transported accordingly by Euler integrating<d-cite key="song2022denoisingdiffusionimplicitmodels"></d-cite> the marginal flow forward or backward in time. The data points will also change in magnitude according to a simple linear schedule, $(1-t)*x_0$, the mean of the Gaussians that define $p_t(x_t|x_0)$. At low timesteps, the path weights are much peakier and $x_t$ will be drawn to its nearest neighbor. Play around, this simulates a “perfectly overfit” diffusion model. For example, try dragging $x_t$ around the points with the slider set to $t=0.10$.</p>

<p>This interpretation sets up Decentralized Diffusion Models very naturally. <b>The marginal flow is a linear system, and linear systems are associative.</b> DDMs exploit this associativity to simplify training systems and improve downstream performance.</p>

<h2 id="decentralized-diffusion-models">Decentralized Diffusion Models</h2>

<p>Decentralized Diffusion Models leverage the associative property of the marginal flow to split training into many independent sub-training jobs focused on producing “flow experts” that each model a subset of the data. These have no data dependencies to each other, so they can be trained wherever compute is available.</p>

<p>We partition the data into K disjoint clusters  ${S_1, S_2, \ldots, S_K}$, and each expert trains on an assigned subset $(x_0 \in S_i)$. Since the marginal flow is a linear combination over data points, we can apply the associative property within each of these data clusters. We therefore rewrite the global marginal flow as a weighted combination of marginal flows over each data partition.</p>

<div class="l-body" style="text-align: center;">
  <img src="/assets/img/decentralized_diffusion/marginal_flow_associated.svg" alt="DDM Overview" style="width: 80%; height: auto;">
</div>

<p>We train a separate diffusion model over each individual data cluster. This is standard flow matching training, so we can reuse popular architectures, hyperparameters and codebases. By adaptively averaging each model’s prediction at test-time, we sample from the entire distribution and optimize the global flow matching objective. We must learn a router to predict the adaptive weights of each expert model at test-time, which we train with a classification objective over the data clusters. We discuss this more thoroughly in the paper.</p>

<p>We can visualize the component flows of a Decentralized Diffusion Model in the plot below. By ensembling them at test-time, we recover the global marginal flow. Drag the black $x_t$ circle to see the denoising predictions for each expert model (blue and red). Slide the time slider to see how the ensembled denoising predictions update the particle $x_t$.</p>

<div class="l-page">
  <iframe src="/assets/plotly/plot_two.html" frameborder="0" scrolling="no" height="620px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>

<p><br></p>

<p>The figure below outlines the data preprocessing, training and inference stages of Decentralized Diffusion Models:</p>

<div class="l-page" style="text-align: center;">
  <img src="/assets/img/decentralized_diffusion/method_wide.jpg" alt="DDM Overview" style="width: 100%; height: auto;">
</div>

<div class="caption l-page">
    <b>Decentralized Diffusion Models follow a three-step training process.</b> We first cluster the dataset using an off-the-shelf representation model (DINOv2). We train a diffusion model over each of these clusters and a router that associates any input $x_t$ with its most likely clusters. At test-time, given a noisy sample, each expert (in red and green) predict their own flows, which combine linearly via the weights predicted by the router. The combined flow samples the entire distribution and is illustrated on the right.
</div>

<h2 id="why-ddms">Why DDMs?</h2>

<p>These are all cute observations, but why does it matter?</p>

<p>Associativity is the key enabler behind many distributed computing algorithms including parallel scans and MapReduce. Decentralized Diffusion Models use the associative property to split diffusion training into many sub-training jobs that proceed independently, with no cross-communication. This means each training job can be assigned to a different cluster in a different location and with different hardware. For example, we train a text-to-image diffusion model on eight independent nodes (8 GPUs each) for around a week. These nodes are readily available to rent from cloud providers, whereas eight nodes with high-bandwidth interconnect must be co-located in one datacenter and are much harder (and more expensive!) to procure.</p>

<div class="l-body" style="text-align: center;">
  <img src="/assets/img/decentralized_diffusion/combined.jpg" alt="DDM Overview" style="width: 100%; height: auto;">
</div>
<div class="caption">
    Some nice generated images from the eight-node training run.
</div>

<p>What’s the performance hit from this added convenience? There is none. In fact, Decentralized Diffusion Models <b>outperform non-decentralized diffusion models FLOP-for-FLOP</b>.</p>

<!-- <div class="l-body" style="text-align: center; margin-top: -240px; margin-bottom: 20px;">
  <img src="/assets/img/decentralized_diffusion/combined_fid_plots.svg" alt="DDM Overview" style="width: 100%; height: auto; clip-path: inset(240px 0 0 0);">
</div> -->
<p><br></p>

<div class="l-body" style="text-align: center; margin-top: 0px; margin-bottom: 20px;">
  <img src="/assets/img/decentralized_diffusion/combined_fid_plots.png" alt="DDM Overview" style="width: 100%; height: auto; clip-path: inset(0px 0 0 0);">
</div>

<div class="caption l-body">
    <b>Comparing DDMs and standard monolithic diffusion models.</b> FLOP-for-FLOP, decentralized diffusion models outperform monolith diffusion models on both ImageNet and LAION Aesthetics.
</div>

<p>By selecting only the most relevant expert model per step at test-time, the ensemble instantiates a sparse model. We can view this as activating only a subset of the parameters of a much larger model, resulting in better performance at the same computational cost. This is also the driving insight in Mixture-of-Experts. We use the same architectures, datasets and training hyperparameters between monoliths and DDMs in all our evaluations, and we account for the additional cost of training the router (~4%). Serving a sparse model can be inconvenient with less sophisticated infrastructure, so we also demonstrate that we can efficiently distill DDMs into monolith models in the paper.</p>

<div class="l-body" style="text-align: center; margin-top: 0px; margin-bottom: 20px;">
  <img src="/assets/img/decentralized_diffusion/laion_scaling_laws.svg" alt="DDM Overview" style="width: 80%; height: auto; clip-path: inset(0 0 0 0);">
</div>

<div class="caption">
    <b>Decentralized diffusion models scale gracefully to billions of parameters.</b> We find that increasing expert model capacity and training compute predictably improves performance.
</div>

<p>Decentralized Diffusion Models also scale gracefully. We see consistent improvements on evaluations as model size and compute capacity increased. Please see the paper for more detailed analysis of DDMs and how they compare to standard diffusion training.</p>

<h2 id="simple-implementation">Simple Implementation</h2>

<p>Decentralized Diffusion Models integrate seamlessly into existing diffusion training environments. In implementation, DDMs involve clustering a dataset then training a standard diffusion model on each cluster. This means nearly everything from existing diffusion infrastructure can be reused. This includes training code, dataloading pipelines, systems optimizations, noise schedules and architectures.</p>

<p>To highlight this, we’ve included a simple code example of how to modify a diffusion training loop to be a DDM in PyTorch.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Inside a standard diffusion training loop:
</span><span class="n">x</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x_t</span> <span class="o">=</span> <span class="nf">forward_diffuse</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span>

<span class="n">pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="n">x_0_pred</span> <span class="o">=</span> <span class="nf">reverse_diffuse</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">noise</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">mse_loss</span><span class="p">(</span><span class="n">x_0_pred</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div>

<p>To make this a DDM, we first cluster the dataset using a representation model. We used DINOv2<d-cite key="oquab2024dinov2learningrobustvisual"></d-cite> and <a href="https://github.com/facebookresearch/MetaCLIP/tree/main/mode" rel="external nofollow noopener" target="_blank">this codebase</a><d-cite key="ma2024modeclipdataexperts"></d-cite> to run k-means clustering on a large dataset. We then train a diffusion model over each cluster. This is completely unchanged from standard diffusion training like above.</p>

<p>The last step is to train a router that predicts the weights of each expert model at test-time. This reduces to a classification objective over the data clusters.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Inside a DDM router training loop:
</span><span class="n">x</span><span class="p">,</span> <span class="n">cluster_idx</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x_t</span> <span class="o">=</span> <span class="nf">forward_diffuse</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span>

<span class="n">pred</span> <span class="o">=</span> <span class="nf">router</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="c1"># shape (B, num_clusters)
</span><span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">cluster_idx</span><span class="p">)</span>
<span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div>

<p>At test-time, we can sample from the entire distribution by ensembling the experts.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Inside a naive DDM inference loop:
</span><span class="n">router_pred</span> <span class="o">=</span> <span class="nf">router</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="c1"># shape (B, num_clusters)
</span><span class="n">router_pred</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">router_pred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">ensemble_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">x_t</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_clusters</span><span class="p">):</span>
    <span class="n">model_pred</span> <span class="o">=</span> <span class="n">models</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">ensemble_pred</span> <span class="o">+=</span> <span class="n">router_pred</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">model_pred</span>

<span class="n">x_t</span> <span class="o">=</span> <span class="nf">reverse_step</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">ensemble_pred</span><span class="p">)</span>
</code></pre></div></div>

<p>We can make this more efficient by inferencing experts in parallel and by using a sparse router that only activates a subset of the experts at test-time. In our comparisons, we actually just select the single most relevant expert model per step at test-time.</p>

<!-- We also add a router that predicts the weights of each expert model at test-time. -->

<h2 id="acknowledgements">Acknowledgements</h2>

<p>We would like to thank Alex Yu for his guidance throughout the project and his score matching derivation. We would also like to thank Daniel Mendelevitch, Songwei Ge, Dan Kondratyuk,  Haiwen Feng, Terrance DeVries, Chung Min Kim, Samarth Sinha, Hang Gao, Justin Kerr and the Luma AI research team for helpful discussions.</p>


      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib"></d-bibliography>

      
      
    </div>

    <!-- Footer -->
    
  <footer class="sticky-bottom mt-5" role="contentinfo">
    

    <div class="container">
      © Copyright 2025
      David
      
      McAllister. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      
      
    </div>
  </footer>


    <!-- Bootsrap & MDB scripts -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>
<!-- <script src="/assets/js/mdb.min.js"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    




    
  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script>


    
  <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>


    

  
</body>
</html>
