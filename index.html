<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Flow Matching Policy Gradients </title> <meta name="author" content="David McAllister"> <meta name="description" content="Simple Online Reinforcement Learning with Flow Matching"> <meta name="keywords" content="Flow Matching Policy Gradients, flow matching, diffusion models, reinforcement learning, PPO"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/fmpg-icon2.jpg?226289fb259539e37275964d8a360baf"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://flowreinforce.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.css" integrity="sha256-q9ba7o845pMPFU+zcAll8rv+gC+fSovKsOoNQ6cynuQ=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github.min.css" integrity="sha256-Oppd74ucMR5a5Dq96FxjEzGF7tTw2fZ/6ksAqDCM8GY=" crossorigin="anonymous" media="screen and (prefers-color-scheme: light)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github-dark.min.css" integrity="sha256-nyCNAiECsdDHrr/s2OQsp5l9XeY2ZJ0rMepjCT2AkBk=" crossorigin="anonymous" media="screen and (prefers-color-scheme: dark)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/css/diff2html.min.css" integrity="sha256-IMBK4VNZp0ivwefSn51bswdsrhk0HoMTLc2GqFHFBXg=" crossorigin="anonymous"> <link defer rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css"> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script>let mermaidTheme=determineComputedTheme();document.addEventListener("readystatechange",()=>{"complete"===document.readyState&&(document.querySelectorAll("pre>code.language-mermaid").forEach(e=>{const t=e.textContent,d=e.parentElement;d.classList.add("unloaded");let a=document.createElement("pre");a.classList.add("mermaid");const n=document.createTextNode(t);a.appendChild(n),d.after(a)}),mermaid.initialize({theme:mermaidTheme}),"undefined"!=typeof d3&&window.addEventListener("load",function(){d3.selectAll(".mermaid svg").each(function(){var e=d3.select(this);e.html("<g>"+e.html()+"</g>");var t=e.select("g"),d=d3.zoom().on("zoom",function(e){t.attr("transform",e.transform)});e.call(d)})}))});</script> <script src="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/js/diff2html-ui.min.js" integrity="sha256-eU2TVHX633T1o/bTQp6iIJByYJEtZThhF9bKz/DcbbY=" crossorigin="anonymous"></script> <script>let diff2HtmlTheme=determineComputedTheme();document.addEventListener("readystatechange",()=>{"complete"===document.readyState&&document.querySelectorAll("pre>code.language-diff2html").forEach(e=>{const t=e.textContent,d=e.parentElement;d.classList.add("unloaded");let l=document.createElement("div");l.classList.add("diff2html"),d.after(l),new Diff2HtmlUI(l,t,{colorScheme:diff2HtmlTheme,drawFileList:!0,highlight:!0,matching:"lines"}).draw()})});</script> <script src="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.js" integrity="sha256-MgH13bFTTNqsnuEoqNPBLDaqxjGH+lCpqrukmXc8Ppg=" crossorigin="anonymous"></script> <script>document.addEventListener("readystatechange",()=>{"complete"===document.readyState&&document.querySelectorAll("pre>code.language-geojson").forEach(e=>{const t=e.textContent,a=e.parentElement;a.classList.add("unloaded");let o=document.createElement("div");o.classList.add("map"),a.after(o);var n=L.map(o);L.tileLayer("https://tile.openstreetmap.org/{z}/{x}/{y}.png",{maxZoom:19,attribution:'&copy; <a href="http://www.openstreetmap.org/copyright">OpenStreetMap</a>'}).addTo(n);let d=L.geoJSON(JSON.parse(t)).addTo(n);n.fitBounds(d.getBounds())})});</script> <script defer src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js" integrity="sha256-0q+JdOlScWOHcunpUk21uab1jW7C1deBQARHtKMcaB4=" crossorigin="anonymous"></script> <script>$(document).ready(function(){var t=null,a=null,e=null,n="";$(".language-chartjs").each(function(){a=$(this),t=$("<canvas></canvas>"),n=a.text(),a.text("").append(t),(e=t.get(0).getContext("2d"))&&n&&new Chart(e,JSON.parse(n))&&a.attr("data-processed",!0)})});</script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js" integrity="sha256-QvgynZibb2U53SsVu98NggJXYqwRL7tg3FeyfXvPOUY=" crossorigin="anonymous"></script> <script>let echartsTheme=determineComputedTheme();document.addEventListener("readystatechange",()=>{"complete"===document.readyState&&document.querySelectorAll("pre>code.language-echarts").forEach(e=>{const t=e.textContent,a=e.parentElement;a.classList.add("unloaded");let r=document.createElement("div");if(r.classList.add("echarts"),a.after(r),"dark"===echartsTheme)var n=echarts.init(r,"dark-fresh-cut");else n=echarts.init(r);n.setOption(JSON.parse(t)),window.addEventListener("resize",function(){n.resize()})})});</script> <script defer src="https://cdn.jsdelivr.net/npm/vega@5.27.0/build/vega.min.js" integrity="sha256-Yot/cfgMMMpFwkp/5azR20Tfkt24PFqQ6IQS+80HIZs=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-lite@5.16.3/build/vega-lite.min.js" integrity="sha256-TvBvIS5jUN4BSy009usRjNzjI1qRrHPYv7xVLJyjUyw=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-embed@6.24.0/build/vega-embed.min.js" integrity="sha256-FPCJ9JYCC9AZSpvC/t/wHBX7ybueZhIqOMjpWqfl3DU=" crossorigin="anonymous"></script> <script>let vegaTheme=determineComputedTheme();document.addEventListener("readystatechange",()=>{"complete"===document.readyState&&document.querySelectorAll("pre>code.language-vega_lite").forEach(e=>{const t=e.textContent,a=e.parentElement;a.classList.add("unloaded");let d=document.createElement("div");d.classList.add("vega-lite"),a.after(d),"dark"===vegaTheme?vegaEmbed(d,JSON.parse(t),{theme:"dark"}):vegaEmbed(d,JSON.parse(t))})});</script> <script defer src="https://tikzjax.com/v1/tikzjax.js" integrity="sha256-+1qyucCXRZJrCg3lm3KxRt/7WXaYhBid4/1XJRHGB1E=" crossorigin="anonymous"></script> <script src="/assets/js/typograms.js?63f3caa50c7a9624f953b3aec207afa6"></script> <script>document.addEventListener("readystatechange",()=>{"complete"===document.readyState&&document.querySelectorAll("pre>code.language-typograms").forEach(e=>{const t=e.textContent,n=e.parentElement.parentElement;let a=document.createElement("pre");a.classList.add("typogram");const d=create("\n"+t,.3,!1);a.appendChild(d),n.appendChild(a),n.removeChild(e.parentElement)})});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}.highlight-python{color:#18327e}.highlight-comment{color:#a31515}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Flow Matching Policy Gradients",
            "description": "Simple Online Reinforcement Learning with Flow Matching",
            "published": "June 20, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/"> <span class="sr-only">(current)</span> </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Flow Matching Policy Gradients</h1> <p>Simple Online Reinforcement Learning with Flow Matching</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#flow-matching">Flow Matching</a> </div> <div> <a href="#on-policy-rl-sample-score-reinforce">On-Policy RL - Sample, Score, Reinforce</a> </div> <div> <a href="#flow-matching-policy-gradients">Flow Matching Policy Gradients</a> </div> </nav> </d-contents> <div style="text-align: center; margin-bottom: 20px;"> <a href="https://mcallisterdavid.com/" style="text-decoration: none; margin: 18px 18px; font-weight: bold;" rel="external nofollow noopener" target="_blank"> David McAllister* </a> <a href="https://songweige.github.io" style="text-decoration: none; margin: 18px 18px; font-weight: bold;" rel="external nofollow noopener" target="_blank"> Songwei Ge* </a> <a href="https://brentyi.github.io/" style="text-decoration: none; margin: 18px 18px; font-weight: bold;" rel="external nofollow noopener" target="_blank"> Brent Yi* </a> <a href="https://hongsukchoi.github.io/" style="text-decoration: none; margin: 18px 18px; font-weight: bold;" rel="external nofollow noopener" target="_blank"> Hongsuk Choi </a> <br> <a href="https://chungmin99.github.io" style="text-decoration: none; margin: 0 10px; font-weight: bold;" rel="external nofollow noopener" target="_blank"> Chung Min Kim </a> <a href="https://ethanweber.me" style="text-decoration: none; margin: 0 10px; font-weight: bold;" rel="external nofollow noopener" target="_blank"> Ethan Weber </a> <a href="https://havenfeng.github.io" style="text-decoration: none; margin: 0 10px; font-weight: bold;" rel="external nofollow noopener" target="_blank"> Haven Feng </a> <a href="https://people.eecs.berkeley.edu/~kanazawa/" style="text-decoration: none; margin: 0 10px; font-weight: bold;" rel="external nofollow noopener" target="_blank"> Angjoo Kanazawa </a> </div> <div> <figure> <video src="/assets/video/fpo_blog_teaser_v2.mp4" class="img-fluid rounded" width="100%" height="100%" autoplay="" loop="" muted=""></video> </figure> </div> <div style="text-align: center; margin-bottom: 20px;"> <a href="https://arxiv.org/pdf/2501.05450" class="btn btn-lg z-depth-0" role="button" style="text-decoration: none; border: 1px solid #ccc; margin: 0 5px; padding: 10px 20px;" rel="external nofollow noopener" target="_blank"> <i class="fas fa-file-pdf"></i> Paper </a> <a href="https://arxiv.org/abs/2501.05450" class="btn btn-lg z-depth-0" role="button" style="text-decoration: none; border: 1px solid #ccc; margin: 0 5px; padding: 10px 20px;" rel="external nofollow noopener" target="_blank"> <i class="ai ai-arxiv"></i> arXiv </a> </div> <p>Flow models have become the go-to approach to model distributions in continuous space. They soak up data with a simple, scalable denoising objective and now represent the state-of-the art in generating images, videos, audio and, more recently, robot action. However, they are still unpopular for learning from rewards through reinforcement learning.</p> <p>Meanwhile, to perform RL in continuous spaces, practicioners typically train far simpler Gaussian policies, which represent a single, ellipsoidal mode of the action distribution. Flow-based policies can capture complex, multimodal action distributions, but they are primarily trained in a supervised manner with behavior cloning (BC). We show that it’s possible to train RL policies using flow matching, the framework behind modern diffusion and flow models, to benefit from its expressivity.</p> <p>We approached this project as researchers primarily familiar with diffusion models. While working on <a href="https://videomimic.net" rel="external nofollow noopener" target="_blank">VideoMimic</a>, we felt limited by the expressiveness of Gaussian policies and thought diffusion could help. In this blog post, we’ll explain how we connect flow matching and on-policy RL in a way that makes sense without an extensive RL background.</p> <p>We introduce <b>Flow Policy Optimization</b> (FPO), a new algorithm to train RL policies with flow matching. It can train expressive flow policies from only rewards. We find its particularly useful to learn underconditioned policies, like humanoid locomotion with simple joystick commands.</p> <h2 id="flow-matching">Flow Matching</h2> <p>Flow matching optimizes a model to transform a simple distribution (e.g., the Gaussian distribution) into a complex one through a multi-step mapping called the marginal flow. We expand on the marginal flow in more detail in another blog post for <a href="https://decentralizeddiffusion.github.io" rel="external nofollow noopener" target="_blank">Decentralized Diffusion Models</a>. The flow smoothly directs a particle $x_t$ to the data distribution, so integrating the flow across time leads to a data sample. We can actually calculate the marginal flow <em>analytically</em>, which we do in real-time in the plot below:</p> <div class="l-page"> <iframe src="/assets/plotly/flow_sde_plot.html" frameborder="0" scrolling="no" width="120%" style="margin-left: -10%; height: auto; min-height: 510px;"></iframe> </div> <p>Each particle here represent an $x_t$ noisy latent that gets iteratively denoised as the time goes from zero to one. If you’re on desktop, drag the control points of the modes on the right to see how the underlying PDF and the particle trajectories change. Notice how the probability mass flows smoothly from the initial noise to form two distinct modes. The multi-step mapping is the magic that lets flow models transform a simple, tractable distribution into one of arbitrary complexity.</p> <p>While it’s simple to compute this flow in 1D, it becomes intractable over large datasets in high dimensional space. Instead, we use flow matching, which compresses the marginal flow into a neural network through a simple reconstruction objective.</p> <div class="l-body" style="text-align: center; margin-top: -4%; margin-bottom: -4%;"> <img src="/assets/img/fpo/flow_matching.svg" alt="DDM Overview" style="margin-left: -5%; width: 110%; height: auto; clip-path: inset(0px 0 0px 0);"> </div> <div class="caption" style="margin-top: 0px; margin-bottom: 2%;"> Flow matching the conditional flow $u_t(x_t|x)$ and velocity prediction $v_t(x_t)$. </div> <p>Flow matching perturbs a clean data sample with Gaussian noise then tasks the model with reconstructing the sample by predicting the velocity, which is the derivative of $x_t$’s position <em>w.r.t.</em> time. In expectation over a fixed dataset, this optimization recovers the marginal flow for any $x_t$. Integrating $x_t$’s position across time along the marginal flow will recover a sample from the data distribution.</p> <p>Geometrically, the marginal flow points to a <em>weighted-average</em> of the data where the weights are a function of the timestep and distance from $x_t$ to each data point. You can see the particles follow the marginal flow exactly in the plot above when stochasticity is turned off. Stated simply, flow matching learns to point the model’s flow field, $v_t(x_t)$, to the data distribution.</p> <p>Flow matching has statistical signifigance too. Instead of computing exact likelihoods (expensive and unstable), it optimizes a lower bound called the Evidence Lower Bound (ELBO). This pushes the model toward higher likelihoods without computing them directly. In the limit, the flow model will sample exactly from the probability distribution of the dataset. So if you’ve learned the flow function well, you’ve learned the underlying structure of the data.</p> <p><b>Flowing toward a data point increases its likelihood under the model.</b></p> <h2 id="on-policy-rl-sample-score-reinforce">On-Policy RL: Sample, Score, Reinforce</h2> <p>On-policy reinforcement learning follows a basic core loop: sample from your policy, score each action with rewards, then make high-reward actions more likely. Rinse and repeat.</p> <p>This procedure climbs the policy gradient—the gradient of expected cumulative reward. Your model collects “experience” from by sampling its learned distribution, sees which samples are most advantageous, and adjusts to perform similar actions more often.</p> <p>On-policy RL can be cast as search iteratively distilled into a model. The policy “happens upon” good behaviors through exploration, then reinforces them. Over time, it discovers the patterns in the random successes and develops reliable strategies. You can start from a pretrained model and continue training with RL to explore within a rich prior rather than at random. This is the dominant approach to upcycle LLMs for preference alignment and reasoning.</p> <h3 id="image-generation-analogy">Image Generation Analogy</h3> <p>Here’s an example RL loop to understand (replace with DMControl rollout videos from Brent):</p> <ul> <li>Generate a batch of images from your model (rollouts)</li> <li>Score each image with a reward (maybe “how much does this look like a dog?”)</li> <li>Train your model to boost the likelihood of high-scoring images</li> <li>Repeat until your model reliably generates high-reward images</li> </ul> <p><b>Sample and score images:</b></p> <div class="l-body" style="text-align: center; margin-top: -0%; margin-bottom: 4%;"> <img src="/assets/img/fpo/dog_rewards.png" alt="DDM Overview" style="margin-left: -1%; width: 102%; height: auto; clip-path: inset(0px 0 0px 0);"> </div> <p>From the rewards, we calculate advantages. These can be viewed as the reward normalized <em>w.r.t.</em> the expected reward from the rest of the rollout under the current policy. This expected reward is what you learn with a critic in PPO<d-cite key="schulman2017proximalpolicyoptimizationalgorithms"></d-cite> or compute as the average of a group’s reward in GRPO<d-cite key="shao2024deepseekmathpushinglimitsmathematical"></d-cite>.</p> <p><b>Calculate each advantage and form policy gradient:</b></p> <div class="l-body" style="text-align: center; margin-top: -0%; margin-bottom: 2%;"> <img src="/assets/img/fpo/dog_adv.png" alt="DDM Overview" style="margin-left: -1%; width: 102%; height: auto; clip-path: inset(0px 0 0px 0);"> </div> <p></p> <p>Given the advantages, train the model on each data point with a gradient update scaled by the corresponding advantage. So, if the advantage is negative, it will become less likely. Postive advantage, more likely.</p> <h2 id="flow-matching-policy-gradients">Flow Matching Policy Gradients</h2> <p>To reiterate, the goal of on-policy RL is simple: increase the likelihood of high-reward actions. Meanwhile, flow matching naturally increases likelihoods by redirecting probability flow toward training samples. This makes our objective clear—<b>redirect the flow toward high reward actions</b>.</p> <p>In the limit of perfect optimization, flow matching assigns probabilities according to the frequency of samples in your training set. Since we’re using RL, that “training set” is dynamically generated from the model.</p> <p>Advantages make the connection between synthetic data generation and on-policy RL explicit. In RL, we calculate the advantage of each sampled action, a quantity that indicates how much better it was than expected. These advantages are centered around zero to reduce variance: positive for better-than-expected actions, negative for worse. Advantages then become a <em>loss weighting</em> in the policy gradient. As a simple example, if an action is very advantageous, the model encounters a scaled-up loss on it and learns to boost it aggressively.</p> <div class="l-body" style="text-align: center;"> <img src="/assets/img/fpo/policy_grad.svg" alt="DDM Overview" style="width: 100%; height: auto; margin-top: 2%;"> </div> <div class="caption"> The policy gradient resembles a standard supervised learning gradient on the model's own synthetic samples with the loss scaled by the reward or advantage (both are valid). </div> <p>Zero-mean advantages are fine for RL in discrete spaces because a negative advantage simply pushes down the logit of a suboptimal action, and the softmax ensures that the resulting action probabilities remain valid and non-negative. Flow matching, however, learns probability flows to sample from a training data distribution. These must be nonnegative by construction, so negative loss weights break the interpretation.</p> <p>There’s a simple solution: make the advantages nonnegative. Shifting advantages by a constant doesn’t change the policy gradient. In fact, this is the mathematical property that lets us use advantages instead of raw rewards in the first place. Here’s how we can understand non-negative advantages in the flow matching framework:</p> <div class="l-body" style="text-align: center;"> <img src="/assets/img/fpo/marginal_flow_fpo.svg" alt="DDM Overview" style="width: 94%; height: auto; margin-top: 2%;"> </div> <div class="caption"> The marginal flow is a linear combination of the (conditional) flow to each data point. The weighting of each path scales with probability of drawing the data point from the dataset, $q(x)$. </div> <p>Advantages manifest as loss-weighting, which can be intuitively expressed in the marginal flow framework. The marginal flow is the weighted average of the paths (the $u_t$’s) from the current noisy particle, $x_t$, to each data point $x$. The paths are also weighed by $q(x)$, the probability of drawing $x$ from your training set. This is typically a constant $\frac{1}{N}$ for a dataset of size $N$, assuming every data point is unique. Loss weights are equivalent to altering the frequency of the data points in your training set. If the loss for a data point is scaled by a factor of 2, its equivalent to that data point showing up twice in the train set.</p> <p>Now, we can get a complete picture of the algorithm that connects flow matching and reinforcement learning:</p> <ul> <li>Generate actions from your flow model using your choice of sampler</li> <li>Score them with rewards and compute advantages</li> <li>Flow match on the advantage-weighed actions</li> </ul> <p>This procedure boosts the likelihood of actions that achieve high reward while preserving the desirable properties of flow models—multimodality, expressivity and the improved exploration that stems from them. We visualize the procedure below in the same 1D flow diagram as above:</p> <div class="l-page"> <iframe src="/assets/plotly/advantage_flow_plot.html" frameborder="0" scrolling="no" width="120%" style="margin-left: -10%; height: auto; min-height: 510px;"></iframe> </div> <h2 id="acknowledgements">Acknowledgements</h2> <p>[TODO: UPDATE]</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib"></d-bibliography> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 David McAllister. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>